{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTPHqthl65wR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.ndimage import rotate\n",
        "from skimage.transform import warp, AffineTransform, resize\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.numpy_ops.np_dtypes import uint8\n",
        "\n",
        "#---------------------------- Dataset Load -------------------------------------\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "\n",
        "\n",
        "data = np.load(\"training_set.npz\")\n",
        "\n",
        "training_set = data[\"training_set\"]\n",
        "X_train = training_set[:, 0]\n",
        "y_train = training_set[:, 1]\n",
        "\n",
        "X_test = data[\"test_set\"]\n",
        "\n",
        "print(\"Dataset loaded!\")\n",
        "print(\"Setting classes...\")\n",
        "\n",
        "category_map = {\n",
        "   0: 0,  # Background\n",
        "   1: 1,  # Soil\n",
        "   2: 2,  # Bedrock\n",
        "   3: 3,  # Sand\n",
        "   4: 4,  # Big rock\n",
        "}\n",
        "\n",
        "#-------------------------- Data Preprocessing ---------------------------------\n",
        "\n",
        "NUM_CLASSES = len(set(category_map.values()))             # Calculate the correct number of classes after mapping\n",
        "print(\"removing outlayers\")\n",
        "\n",
        "mask_alien = y_train[142]                                 # Alien image mask\n",
        "\n",
        "X_cleaned = []                                            # Initialize lists to store the cleaned dataset\n",
        "y_cleaned = []\n",
        "\n",
        "for idx, mask in enumerate(y_train):\n",
        "   if np.array_equal(mask, mask_alien):                    # Check if the mask is the alien image mask\n",
        "       continue\n",
        "\n",
        "   X_cleaned.append(X_train[idx])                          # If it's not a duplicate, add the image and its corresponding mask to the cleaned dataset\n",
        "   y_cleaned.append(mask)\n",
        "\n",
        "X_cleaned = np.array(X_cleaned)                            # Convert the cleaned lists to numpy arrays\n",
        "y_cleaned = np.array(y_cleaned)\n",
        "\n",
        "print(\"outlayers removed\")\n",
        "print(\"Classes setted!\")\n",
        "\n",
        "#------------------------- Train-Test Split ------------------------------------\n",
        "\n",
        "print(\"Splitting dataset...\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_cleaned, y_cleaned, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "print(\"Data splitted!\")\n",
        "\n",
        "print(\"Building model...\")\n",
        "\n",
        "#---------------------- U-Net + pyramidal pooling ------------------------------\n",
        "\n",
        "def pyramidal_pooling_module(input, pool_sizes=[2, 4, 8]):\n",
        "  pooled1 = tfkl.GlobalAveragePooling2D()(input)\n",
        "  pooled1 = tfkl.Reshape([ 1, 1, pooled1.shape[-1]])(pooled1)\n",
        "  upsampled1 = tfkl.UpSampling2D(size=(16,32),interpolation = 'bilinear')(pooled1)      # GAP for global context\n",
        "\n",
        "  pooled2 = tfkl.AveragePooling2D(pool_size = (8,8))(input)                             # wide pool\n",
        "  upsampled2 = tfkl.UpSampling2D(size=(8,8),interpolation = 'bilinear')(pooled2)\n",
        "\n",
        "  pooled3 = tfkl.AveragePooling2D(pool_size = (4,4))(input)                             # mid pool\n",
        "  upsampled3 = tfkl.UpSampling2D(size=(4,4),interpolation = 'bilinear')(pooled3)\n",
        "\n",
        "  pooled4 = tfkl.AveragePooling2D(pool_size = (2,2))(input)                             # small pool\n",
        "  upsampled4 = tfkl.UpSampling2D(size=(2,2),interpolation = 'bilinear')(pooled4)\n",
        "\n",
        "  concat = tfkl.Concatenate()([upsampled1,upsampled2,upsampled3,upsampled4])\n",
        "\n",
        "  output = tfkl.Conv2D(64, (1, 1), padding='same')(concat)                              # convolution 1x1 to reduce dimensionality\n",
        "  conv1 = tfkl.BatchNormalization()(output)\n",
        "  conv1 = tfkl.ReLU()(conv1)\n",
        "\n",
        "  return conv1\n",
        "\n",
        "\n",
        "def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2):\n",
        "\n",
        "   x = input_tensor\n",
        "\n",
        "   for i in range(stack):                                                       # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n",
        "       x = tfkl.Conv2D(filters * 2, kernel_size=kernel_size, padding='same')(x)\n",
        "       x = tfkl.BatchNormalization()(x)\n",
        "       x = tfkl.Activation(activation)(x)\n",
        "\n",
        "   # Return the transformed tensor\n",
        "   return x\n",
        "\n",
        "\n",
        "def get_unet_model(input_shape=(64, 128, 1), num_classes=NUM_CLASSES):          # UNet model\n",
        "   input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "   down_block_1 = unet_block(input_layer, 16)                                   # Down sampling path\n",
        "   d1 = tfkl.MaxPooling2D()(down_block_1)\n",
        "\n",
        "   down_block_2 = unet_block(d1, 32)\n",
        "   d2 = tfkl.MaxPooling2D()(down_block_2)\n",
        "\n",
        "   bottleneck = unet_block(d2, 64)                                              # Bottleneck\n",
        "   ppm = pyramidal_pooling_module(bottleneck)                                   # Pyramid Pooling Module on bottleneck\n",
        "\n",
        "   u1 = tfkl.UpSampling2D(size=(2,2),interpolation = 'bilinear')(ppm)           # Upsampling path\n",
        "   u1 = tfkl.Concatenate()([u1, down_block_2])                                  # skip connection\n",
        "   u1 = unet_block(u1, 32)\n",
        "\n",
        "   u2 = tfkl.UpSampling2D(size=(2,2),interpolation = 'bilinear')(u1)\n",
        "   u2 = tfkl.Concatenate()([u2, down_block_1])\n",
        "   u2 = unet_block(u2, 16)\n",
        "\n",
        "   output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\")(u2)    #output softmax layer\n",
        "\n",
        "   model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "   return model\n",
        "\n",
        "\n",
        "model = get_unet_model()\n",
        "\n",
        "#------------------------ IoU metric definition --------------------------------\n",
        "\n",
        "print(\"Model built!\")\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "   y_true = tf.cast(y_true, tf.float32)\n",
        "   y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "   # Converte i tensori in valori booleani (0 o 1) per il calcolo\n",
        "   intersection = tf.reduce_sum(y_true * y_pred)\n",
        "   union = tf.reduce_sum(y_true + y_pred) - intersection\n",
        "\n",
        "   return intersection / (union + tf.keras.backend.epsilon())\n",
        "\n",
        "\n",
        "def iou_multi_class(y_true, y_pred, num_classes):\n",
        "   iou_per_class = []\n",
        "   y_pred = tf.argmax(y_pred, axis=-1)\n",
        "   for i in range(num_classes):\n",
        "       y_true_class = tf.equal(y_true, i)\n",
        "       y_pred_class = tf.equal(y_pred, i)\n",
        "       iou_per_class.append(iou(y_true_class, y_pred_class))\n",
        "\n",
        "   return tf.reduce_mean(iou_per_class)\n",
        "\n",
        "#------------------------- Model compile and train -----------------------------\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "\n",
        "patience = 20\n",
        "epochs = 1000\n",
        "\n",
        "print(\"Parameters setted!\")\n",
        "\n",
        "mean_iou = tfk.metrics.MeanIoU(num_classes=NUM_CLASSES, ignore_class=0, sparse_y_pred=False)  # Define the MeanIoU ignoring the background class\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[mean_iou])\n",
        "\n",
        "print(\"Model compiled!\")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(                              # Setup callbacks\n",
        "   monitor='val_mean_io_u',\n",
        "   mode='max',\n",
        "   patience=patience,\n",
        "   restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"Training model...\")\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
        "\n",
        "#---------------------------- Local Testing ------------------------------------\n",
        "\n",
        "y_predicted = model.predict(X_val)\n",
        "final_val_meanIoU = iou_multi_class(y_val, y_predicted, NUM_CLASSES)\n",
        "\n",
        "print(\"Model trained!\")\n",
        "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#----------------------------- Loss Plot ---------------------------------------\n",
        "\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Validation Loss durante l\\'addestramento')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "\n",
        "#----------------------------- Model Save --------------------------------------\n",
        "\n",
        "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "model_filename = f\"model_{timestep_str}.keras\"\n",
        "\n",
        "model.save(model_filename)\n",
        "del model\n",
        "\n",
        "print(\"Model saved!\")\n",
        "\n",
        "import pandas as pd\n",
        "from keras.src.saving import load_model\n",
        "\n",
        "model_filename = \"model_241209_222900.keras\"\n",
        "model = load_model(model_filename)\n",
        "\n",
        "\n",
        "# Function to predict in batches and save directly\n",
        "def predict_and_save_in_batches(model, X_test, batch_size, filename):\n",
        "    n_samples = len(X_test)\n",
        "    n_batches = (n_samples + batch_size - 1) // batch_size  # Compute number of batches\n",
        "\n",
        "    with open(filename, 'w') as file:\n",
        "        header_written = False\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min((batch_idx + 1) * batch_size, n_samples)\n",
        "\n",
        "            # Predict the batch\n",
        "            batch_preds = model.predict(X_test[start_idx:end_idx])\n",
        "            batch_preds = np.argmax(batch_preds, axis=-1)\n",
        "            batch_preds = batch_preds.astype('float16')  # Save memory\n",
        "\n",
        "            # Convert batch predictions to DataFrame\n",
        "            batch_df = pd.DataFrame(batch_preds.reshape(batch_preds.shape[0], -1))\n",
        "            batch_df[\"id\"] = np.arange(start_idx, end_idx)\n",
        "            cols = [\"id\"] + [col for col in batch_df.columns if col != \"id\"]\n",
        "            batch_df = batch_df[cols]\n",
        "\n",
        "            # Write batch to CSV\n",
        "            batch_df.to_csv(file, index=False, header=(not header_written))\n",
        "            header_written = True  # Ensure the header is written only once\n",
        "\n",
        "\n",
        "# Parameters\n",
        "batch_size = 256  # Adjust based on your system's memory capacity\n",
        "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
        "submission_filename = f\"submission_{timestep_str}.csv\"\n",
        "\n",
        "# Call the function to predict and save in batches\n",
        "predict_and_save_in_batches(model, X_test, batch_size, submission_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
