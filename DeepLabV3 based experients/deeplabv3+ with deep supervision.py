# -*- coding: utf-8 -*-
"""DeepLabV3+ deep_supervision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cbEK4dJgdq709mgIN-aCxtFUiZ4uGtK
"""

import random
from datetime import datetime
import numpy as np
import tensorflow as tf
from scipy.ndimage import rotate
from skimage.transform import warp, AffineTransform, resize
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
from sklearn.model_selection import train_test_split
from tensorflow.python.ops.numpy_ops.np_dtypes import uint8
import pandas as pd
from keras.src.saving import load_model
#----------------------------- Load Data ---------------------------------------

print("Loading dataset...")


data = np.load("training_set.npz")

training_set = data["training_set"]
X_train = training_set[:, 0]
y_train = training_set[:, 1]

X_test = data["test_set"]

print("Dataset loaded!")
print("Setting classes...")

category_map = {
   0: 0,  # Background
   1: 1,  # Soil
   2: 2,  # Bedrock
   3: 3,  # Sand
   4: 4,  # Big rock
}

NUM_CLASSES = len(set(category_map.values()))           # Calculate the correct number of classes after mapping
print("removing outlayers")

mask_alien = y_train[142]                               # Alien image mask

X_cleaned = []                                          # Initialize lists to store the cleaned dataset
y_cleaned = []

for idx, mask in enumerate(y_train):
   if np.array_equal(mask, mask_alien):                 # Check if the mask is the alien image mask
       continue                                         # If it is a duplicate mask, skip this image and mask

   X_cleaned.append(X_train[idx])                       # If it's not a duplicate, add the image and its corresponding mask to the cleaned dataset
   y_cleaned.append(mask)

X_cleaned = np.array(X_cleaned)                         # Convert the cleaned lists to numpy arrays
y_cleaned = np.array(y_cleaned)

print("outlayers removed")
print("Classes setted!")

#--------------------------- Train-Val Split -----------------------------------

print("Splitting dataset...")

X_train, X_val, y_train, y_val = train_test_split(X_cleaned, y_cleaned, test_size=0.1, random_state=42, shuffle=True)

print("Data splitted!")

#---------------------- Dice Loss implementation -------------------------------

def dice_loss(y_true, y_pred):

  num_classes = y_pred.shape[-1]
  dice_scores = []

  for i in range(num_classes):

      y_true_class = tf.cast(tf.equal(y_true, i), tf.float32)
      y_pred_class = y_pred[..., i]

      numerator = 2 * tf.reduce_sum(y_true_class * y_pred_class) + 1e-7
      denominator = tf.reduce_sum(y_true_class + y_pred_class) + 1e-7
      dice_score = 1 - numerator / denominator
      dice_scores.append(dice_score)

      mean_dice_loss = tf.reduce_mean(dice_scores)
      return mean_dice_loss

#-------------- Combined loss for deep supervision -----------------------------

def combined_loss(y_true, y_pred):

    y_pred_main, y_pred_aux1, y_pred_aux2, y_pred_aux3 = y_pred[0], y_pred[1], y_pred[2], y_pred[3]

    loss_main = dice_loss(y_true, y_pred_main)
    loss_aux1 = dice_loss(y_true, y_pred_aux1)
    loss_aux2 = dice_loss(y_true, y_pred_aux2)
    loss_aux3 = dice_loss(y_true, y_pred_aux3)
    return loss_main + 0.5 * loss_aux1 + 0.25 * loss_aux2 + 0.125 * loss_aux3

#-------------- DeepLabV3+ deep_supervision ------------------------------------

def conv_block(input_layer,kernel_size,dropout,filters, dilation_rate):
  c1 = tfkl.Conv2D(filters , kernel_size=kernel_size , padding='same',dilation_rate = dilation_rate, kernel_initializer="he_normal")(input_layer)
  c1 = tfkl.BatchNormalization()(c1)
  c1 = tfkl.ReLU()(c1)
  c1 = tfkl.Dropout(dropout)(c1)
  return c1

def appm_block(input,filter):
  appm1 = conv_block(input,1,0,filter,1,)
  appm2 = conv_block(input,3,0,filter,1)
  appm3 = conv_block(input,3,0,filter,2)
  appm4 = conv_block(input,3,0,filter,3)
  appm_glob = tfkl.AveragePooling2D(pool_size = (16,32))(input)

  appm_glob = tfkl.UpSampling2D(size = (16,32), interpolation = 'bilinear')(appm_glob)

  appm = tfkl.Concatenate()([appm1,appm2,appm3,appm4,appm_glob])
  appm = conv_block(appm,1,0,filter,1)

  return appm

def model(input, kernel_size):
  filters = 32
  dropout = 0
  input_layer = tfkl.Input(shape=input, name='input_layer')

  c1 = conv_block(input_layer,kernel_size,dropout,filters,1)
  c1 = conv_block(c1,kernel_size,dropout,filters,1)
  p1 = tfkl.MaxPooling2D()(c1)

  c2= conv_block(p1,kernel_size,dropout,filters*2,1)
  c2 = conv_block(c2,kernel_size,dropout,filters*2,1)
  p2 = tfkl.MaxPooling2D()(c2)

  c3 = conv_block(p2,kernel_size,dropout,filters*4,1)
  c3 = conv_block(c3,kernel_size,dropout,filters*4,1)

  appm = appm_block(c3,filters*4)

  hlf = conv_block(c3,1,0,128,1)
  mlf = conv_block(c2,1,0,128,1)
  llf = conv_block(c1,1,0,128,1)

  hlf = tfkl.UpSampling2D(size = (2,2), interpolation = 'bilinear')(hlf)
  hlf_out = tfkl.UpSampling2D(size = (2,2), interpolation = 'bilinear')(hlf)
  hlf_mlf = tfkl.Add()([hlf, mlf])
  hlf_mlf = conv_block(hlf_mlf,3,0,filters*4,1)

  hlf_mlf = tfkl.UpSampling2D(size = (2,2), interpolation = 'bilinear')(hlf_mlf)
  hlf_mlf_llf = tfkl.Add()([hlf_mlf, llf])
  hlf_mlf_llf = conv_block(hlf_mlf_llf,3,0,filters*2,1)

  appm = tfkl.UpSampling2D(size = (4,4), interpolation = 'bilinear')(appm)
  hlf_mlf_llf_appm = tfkl.Add()([hlf_mlf_llf, appm])
  hlf_mlf_llf_appm = conv_block(hlf_mlf_llf_appm,3,0,filters,1)

  main_output = tfkl.Conv2D( 5 , kernel_size=1, padding='same', activation="softmax")(hlf_mlf_llf_appm)
  aux_output1 = tfkl.Conv2D( 5 , kernel_size=1, padding='same', activation="softmax")(hlf_mlf_llf)
  aux_output2 = tfkl.Conv2D( 5 , kernel_size=1, padding='same', activation="softmax")(hlf_mlf)
  aux_output3 = tfkl.Conv2D( 5 , kernel_size=1, padding='same', activation="softmax")(hlf_out)

  model = tf.keras.Model(inputs=input_layer, outputs=[main_output, aux_output1, aux_output2, aux_output3])
  return model

model = model((64,128,1), 3)

model.summary()

#----------------------- Mean IoU Metric ---------------------------------------

def iou(y_true, y_pred):
   y_true = tf.cast(y_true, tf.float32)
   y_pred = tf.cast(y_pred, tf.float32)

   intersection = tf.reduce_sum(y_true * y_pred)
   union = tf.reduce_sum(y_true + y_pred) - intersection

   return intersection / (union + tf.keras.backend.epsilon())


def iou_multi_class(y_true, y_pred, num_classes):
   iou_per_class = []
   y_pred = tf.argmax(y_pred, axis=-1)
   for i in range(num_classes):
       y_true_class = tf.equal(y_true, i)
       y_pred_class = tf.equal(y_pred, i)
       iou_per_class.append(iou(y_true_class, y_pred_class))

   return tf.reduce_mean(iou_per_class)

print("Compiling model...")

patience = 20
epochs = 1000

print("Parameters setted!")
mean_iou = tfk.metrics.MeanIoU(num_classes=NUM_CLASSES, ignore_class=0, sparse_y_pred=False)  # Define the MeanIoU ignoring the background class

#----------------------- Model train -------------------------------------------

model.compile(optimizer="adam",
              loss=combined_loss,
              loss_weights=[1, 0.5, 0.25, 0.125],
              metrics=[mean_iou, mean_iou, mean_iou, mean_iou],
              run_eagerly=True)

print("Model compiled!")

# Setup callbacks
early_stopping = tf.keras.callbacks.EarlyStopping(
   monitor='val_conv2d_17_mean_io_u',
   mode='max',
   patience=patience,
   restore_best_weights=True
)

print("Training model...")

# Train the model
history = model.fit(X_train,
                    [y_train,y_train,y_train,y_train],
                    validation_data=(X_val, [y_val,y_val,y_val,y_val]),
                    batch_size=8,
                    epochs=epochs,
                    callbacks=[early_stopping])

# Calculate and print the final validation accuracy
y_predicted = model.predict(X_val)
final_val_meanIoU = iou_multi_class(y_val, y_predicted, NUM_CLASSES)

print("Model trained!")
print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}')

#----------------------------- Model Save --------------------------------------

timestep_str = datetime.now().strftime("%y%m%d_%H%M%S")
model_filename = f"model_{timestep_str}.keras"

model.save(model_filename)
del model

print("Model saved!")


model_filename = "model_241210_133740.keras"
model = load_model(model_filename)


# Function to predict in batches and save directly
def predict_and_save_in_batches(model, X_test, batch_size, filename):
    n_samples = len(X_test)
    n_batches = (n_samples + batch_size - 1) // batch_size  # Compute number of batches

    with open(filename, 'w') as file:
        header_written = False
        for batch_idx in range(n_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, n_samples)

            # Predict the batch
            batch_preds = model.predict(X_test[start_idx:end_idx])
            batch_preds = np.argmax(batch_preds, axis=-1)
            batch_preds = batch_preds.astype('float16')  # Save memory

            # Convert batch predictions to DataFrame
            batch_df = pd.DataFrame(batch_preds.reshape(batch_preds.shape[0], -1))
            batch_df["id"] = np.arange(start_idx, end_idx)
            cols = ["id"] + [col for col in batch_df.columns if col != "id"]
            batch_df = batch_df[cols]

            # Write batch to CSV
            batch_df.to_csv(file, index=False, header=(not header_written))
            header_written = True  # Ensure the header is written only once