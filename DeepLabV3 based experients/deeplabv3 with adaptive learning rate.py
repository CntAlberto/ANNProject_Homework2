# -*- coding: utf-8 -*-
"""DeepLabV3_and_Adaptive_LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GR-8-D5-4HCjmBfjDHWrBycU3GcFSNOd
"""

# Commented out IPython magic to ensure Python compatibility.
import random
from datetime import datetime
import numpy as np
import tensorflow as tf
from scipy.ndimage import rotate
from skimage.transform import warp, AffineTransform, resize
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
from sklearn.model_selection import train_test_split
from tensorflow.python.ops.numpy_ops.np_dtypes import uint8

#---------------------------- Dataset Load -------------------------------------

print("Loading dataset...")

data = np.load("training_set.npz")

training_set = data["training_set"]

X_train = training_set[:, 0]
y_train = training_set[:, 1]

X_test = data["test_set"]


print("Dataset loaded!")

print("Setting classes...")


category_map = {
   0: 0,  # Background
   1: 1,  # Soil
   2: 2,  # Bedrock
   3: 3,  # Sand
   4: 4,  # Big rock
}

#----------------------------- Pre-Processing ----------------------------------

NUM_CLASSES = len(set(category_map.values()))             # setting number of classes

print("removing outliers")

mask_alien = y_train[142]                                 # Alien image mask for outlier removal

X_cleaned = []
y_cleaned = []

for idx, mask in enumerate(y_train):
   if np.array_equal(mask, mask_alien):                   # If it is a duplicate mask, skip this image and mask
       continue

   X_cleaned.append(X_train[idx])                         # If it's not a duplicate, add the image and its corresponding mask to the cleaned dataset
   y_cleaned.append(mask)

X_cleaned = np.array(X_cleaned)                           # Convert the cleaned lists to numpy arrays
y_cleaned = np.array(y_cleaned)


print("outliers removed")
print("Classes setted!")

#--------------------------- Train - Test Split --------------------------------

print("Splitting dataset...")

X_train, X_val, y_train, y_val = train_test_split(X_cleaned, y_cleaned, test_size=0.1, random_state=42, shuffle=True)

print("Data splitted!")

#-------------------------- Image Augmentation ---------------------------------


MULTIPLIER_C4 = 30                                                # number of augmented images per original image
MAX_TRANSLATION = 10

    #Class balancing (with random flipping, rotations and traslations):

unique_labels, counts = np.unique(y_train, return_counts=True)    # Compute unique labels and their pixel counts in the masks
print(f"Class distribution: {dict(zip(unique_labels, counts))}")

    # Define a threshold for underrepresented classes

class_imbalance_threshold = 30000                                 # Pixel count threshold
underrepresented_classes = unique_labels[counts < class_imbalance_threshold]
print(f"Underrepresented classes: {underrepresented_classes}")

    # Augmentation functions

def augment_flipping(image, mask):
    image = np.fliplr(image)
    mask = np.fliplr(mask)
    return image, mask

def random_translate(image, mask, max_translation=MAX_TRANSLATION):
    height, width = image.shape[:2]                         # Get the height and width of the image

    # Generate random traslations in x and y directions

    tx = random.randint(-max_translation, max_translation)  # Traslation in x direction
    ty = random.randint(-max_translation, max_translation)  # Traslation in y direction

    transform = AffineTransform(translation=(tx, ty))
    image = warp(image, transform, mode='reflect', cval=0)
    mask = warp(mask, transform, mode='reflect', cval=0)

    return image, mask

    # Augment the dataset

augmented_images = []
augmented_masks = []

for i, (img, mask) in enumerate(zip(X_train, y_train)):
    unique_labels = np.unique(mask)
    needs_augmentation = any(label in underrepresented_classes for label in unique_labels)
    if needs_augmentation:
        for j in range(0,MULTIPLIER_C4):                    # create 5 augmented images from the same original image
            img, mask = augment_flipping(img, mask)
            img, mask = random_translate(img, mask)
            augmented_images.append(img)
            augmented_masks.append(mask)

    # Convert lists to arrays

augmented_images = np.array(augmented_images, dtype=np.uint8)
augmented_masks = np.array(augmented_masks, dtype=np.uint8)
print(f"Augmented images set size: {augmented_images.shape}")


print(f"Original training set size: {X_train.shape}")

    # Add to training set

X_train = np.concatenate((X_train, augmented_images), axis=0)
y_train = np.concatenate((y_train, augmented_masks), axis=0)
print(f"Augmented training set size: {X_train.shape}")

#------------------------- DeeplabV3+ model adapted ----------------------------


def conv_block(input_layer,kernel_size,dropout,filters, dilation_rate):         # convolutional block with one convolutional layer
  c1 = tfkl.Conv2D(filters , kernel_size=kernel_size , padding='same',dilation_rate = dilation_rate, kernel_initializer="he_normal")(input_layer)
  c1 = tfkl.BatchNormalization()(c1)
  c1 = tfkl.ReLU()(c1)
  c1 = tfkl.Dropout(dropout)(c1)
  return c1

def appm_block(input):                                                          # appm block for multi-scale analysis (since our images have low resolution
  appm1 = conv_block(input,1,0,256,1,)                                          # we adopted low dilations rates)
  appm2 = conv_block(input,3,0,256,1)
  appm3 = conv_block(input,3,0,256,2)
  appm4 = conv_block(input,3,0,256,3)
  appm_glob = tfkl.AveragePooling2D(pool_size = (16,32))(input)

  appm_glob = tfkl.UpSampling2D(size = (16,32), interpolation = 'bilinear')(appm_glob)

  appm = tfkl.Concatenate()([appm1,appm2,appm3,appm4,appm_glob])
  appm = conv_block(appm,1,0,128,1)

  return appm

def model(input, kernel_size):                                                  # model definition
  filters = 32
  dropout = 0
  input_layer = tfkl.Input(shape=input, name='input_layer')

  c1 = conv_block(input_layer,kernel_size,dropout,filters,1)
  c1 = conv_block(c1,kernel_size,dropout,filters,1)
  p1 = tfkl.MaxPooling2D()(c1)

  c2= conv_block(p1,kernel_size,dropout,filters*2,1)
  c2 = conv_block(c2,kernel_size,dropout,filters*2,1)
  p2 = tfkl.MaxPooling2D()(c2)

  c3 = conv_block(p2,kernel_size,dropout,filters*4,1)                           # only two pooling layers in order to don't reduce too much resolution
  c3 = conv_block(c3,kernel_size,dropout,filters*4,1)

  appm = appm_block(c3)

  hlf = conv_block(c3,1,0,128,1)                                                # high semantic low spatial level features
  mlf = conv_block(c2,1,0,128,1)                                                # mid semantic mid spatial level features
  llf = conv_block(c1,1,0,128,1)                                                # low semantic high spatial level features

  hlf = tfkl.UpSampling2D(size = (2,2), interpolation = 'bilinear')(hlf)        # upsample and through add
  hlf_mlf = tfkl.Add()([hlf, mlf])
  hlf_mlf = conv_block(hlf_mlf,3,0,128,1)                                       # convolutional block to reduce artifacts due to upsampling and addition

  hlf_mlf = tfkl.UpSampling2D(size = (2,2), interpolation = 'bilinear')(hlf_mlf)
  hlf_mlf_llf = tfkl.Add()([hlf_mlf, llf])
  hlf_mlf_llf = conv_block(hlf_mlf_llf,3,0,128,1)

  appm = tfkl.UpSampling2D(size = (4,4), interpolation = 'bilinear')(appm)      # high level multi-scale feature information from atrous pyramidal pooling module
  hlf_mlf_llf_appm = tfkl.Add()([hlf_mlf_llf, appm])
  hlf_mlf_llf_appm = conv_block(hlf_mlf_llf_appm,3,0,128,1)

  output_layer = tfkl.Conv2D( 5 , kernel_size=1, padding='same', activation="softmax")(hlf_mlf_llf_appm)  # softmax output layer

  model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
  return model


model = model((64,128,1), 3)

#------------------------ IoU metric definition --------------------------------

def iou(y_true, y_pred):
   y_true = tf.cast(y_true, tf.float32)
   y_pred = tf.cast(y_pred, tf.float32)

   intersection = tf.reduce_sum(y_true * y_pred)
   union = tf.reduce_sum(y_true + y_pred) - intersection

   return intersection / (union + tf.keras.backend.epsilon())


def iou_multi_class(y_true, y_pred, num_classes):
   iou_per_class = []
   y_pred = tf.argmax(y_pred, axis=-1)
   for i in range(num_classes):
       y_true_class = tf.equal(y_true, i)
       y_pred_class = tf.equal(y_pred, i)
       iou_per_class.append(iou(y_true_class, y_pred_class))

   return tf.reduce_mean(iou_per_class)

#----------------------- Model compiling and model fit -------------------------

print("Compiling model...")

patience = 30
epochs = 1000

print("Parameters setted!")

mean_iou = tfk.metrics.MeanIoU(num_classes=NUM_CLASSES, ignore_class=0, sparse_y_pred=False)
model.compile(optimizer=Adam(learning_rate=0.005), loss="sparse_categorical_crossentropy", metrics=[mean_iou])
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=10, min_lr=1e-6, verbose=1)

print("Model compiled!")

#--------------------------------- Training ------------------------------------

early_stopping = tf.keras.callbacks.EarlyStopping(
   monitor='val_mean_io_u',
   mode='max',
   patience=patience,
   restore_best_weights=True
)

print("Training model...")

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])


y_predicted = model.predict(X_val)                                              # Calculate and print the final validation accuracy
final_val_meanIoU = iou_multi_class(y_val, y_predicted, NUM_CLASSES)

print("Model trained!")
print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}')

#----------------------------- Save the model ----------------------------------

timestep_str = datetime.now().strftime("%y%m%d_%H%M%S")
model_filename = f"model_{timestep_str}.keras"

model.save(model_filename)
del model

print("Model saved!")